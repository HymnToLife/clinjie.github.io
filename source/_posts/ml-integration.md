title: 集成学习
date: 2017-03-07 13:18:42
tags: ML
categories: ML
---

# 个体集成 #

集成学习通过构建并结合多个学习器来完成学习任务，先产生一组个体学习器，再用某种策略把他们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生。

同质集成：个体全是相同类型的学习器，称为基学习器

异质集成：个体可以是不同的学习器，称为组件学习器

根据个体学习器的生成方式，目前的集成学习方法大致分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法Boosting；以及不存在强依赖关系，可同时生成的并行化方法Bagging和随机森林。

<!--more-->

欲构建泛化能力强的集成，getInstance学习器应该好而不同。

# Boosting #

先从初始训练集中训练一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注。然后基于调整后的样本分布来训练下一个基学习器，如此重复进行，直至基学习器数目达到事先制定的值T，最终将这T个基学习器进行加权结合。

## AdaBoost ##

假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。

AdaBoost反复学习基本分类器，在每一轮,=1,2,...,M顺次的执行下面的操作：

1. 使用当前分布$D\_m$加权的训练数据集，学习基本分类器$G\_m(x)$

2. 计算基本分类器$G\_m(x)$在加权训练数据集上的分类误差率：$e\_m=p(G\_m(x\_i) \neq y\_i)=\sum\limits\_{G\_m(x\_i)\neq y\_i}\omega\_{mi}$

这里，$\omega\_{mi}$表示第m轮中第i个实例的权值，$\sum\limits\_{i=1}^N\omega\_{mi}=1$

3. 计算基本分类器$G\_m(x)$的系数$\alpha\_m$表示$G\_m(x)$在最终分类器中的重要性。当上面计算的$e\_m \leq \frac{1}{2}$时，$\alpha \geq 0$，并且随着分类误差的减小增大，所以**分类误差率越小的基本分类器在最终分类器中的作用越大**

4. 更新训练数据的权值分布为下一轮做准备,被基本分类器$G\_m(x)$误分类样本的权值得到扩大，所以**误分类样本在下一轮学习中起更大更大作用**

最后通过线性组合实现M个基本分类器的加权表决，$\alpha\_m$表示了基本分类器的重要性

# 并行化方法 #

## Bagging ##

Bagging基于自助取样法，给定包含m个样本的数据集，先随机取出一个样本放入采样集中，在吧该样本放回到初始数据集合中，使得下次采样时候仍有可能被选中。经过m次随机采样操作，得到含有m个样本的采样集。

根据上面的自助采样方法，我们得到T个含有m个训练样本（m个训练样本很有可能有重复的），然后基于每个采样集训练一个基学习器。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测出现同样票数情况，可以随机选择一个。

## 随机森林 ##

随机森林（Random Forest，RF）是Bagging的一个变体。RF在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。

传统决策树在选择划分属性时是在当前节点的属性集合（假定有d个属性中选择一个最优属性），而在RF中对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k属性的子集，然后再从这个子集中选择一个最优属性用于划分。k决定了随机性，一般推荐$k=log\_2 d$
