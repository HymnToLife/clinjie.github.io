title: 模型评估与选择
date: 2017-03-1 12:05:50
toc: true
tags: ML
categories: ML
---

# 误差与拟合 #

## 误差 ##

学习器的实际预测输出与样本的真是输出之间的差异称为"误差"

学习器在训练集上的误差叫"训练误差"

学习器在新样本的误差叫"泛化误差"（重要）
<!--more-->
## 拟合 ##

我们实际希望的，是在新样本上能表现得很好的学习器.为了达到这个 目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，这样才能在遇到新样本时做出正确的判别。

然而，当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的普适性质，这样就会导致泛化性能下降。这种现象在机器学习中称为“过合”（overfitting).

与“过拟合”相对的是“欠拟合”（underfitting),这是指对训练样本的一般性质尚未学好。

# 评估方法 #

## 留出法 ##

将整体数据划分为两个互斥的数据集合，分别作为测试集与训练集。注意测试集与训练集要尽量保证层次、类别上的相似。可以进行多次留出法，然后取误差的平均值。一般取2/3或者3/4的数据作为训练集。

## 交叉验证 ##

1. 同样是将数据集合互斥划分，这里互斥的划分为k份，相互之间保持层次分布的平衡。
2. 每次使用k-1份数据作为训练集，余下的作为预测集
3. 进行k次实验，取平均误差值

## 自助法 ##

上述的两种方法有一个共同的缺陷就是都把训练集合与整体的样本集合规模有偏差，而且由于每次的偏差不同，造成无法预料的影响。

自助法：给定包含m个样本的数据集D，对他进行采样产生数据集d。每次随机的从D中跳一个样本复制到d中，进行m次，生成了包含m个样本的数据集d。显然，d中可能包含多个重复的样本数据。经过计算，m取无限大时，d中大概有D的63%数据。

之后我们使用d作为训练样本，而D\d作为测试集合。这样，实际的训练与测试都使用了m个样本，且保证了D\d有3成多的非训练样本。


# 性能度量 #

这里主要介绍两个属性：P（查准率precision）与R（查全率recall）

1. 查全率＝（检索出的相关信息量/系统中的相关信息总量）*100%  用户感兴趣的信息中有多少被检索出来
2. 查准率＝（检索出的相关信息量/检索出的信息总量）*100%  检索出的信息有多少比例是用户感兴趣的

查全率是衡量检索系统和检索者检出相关信息的能力，查准率是衡量检索系统和检索者拒绝非相关信息的能力。

实验证明，在查全率和查准率之间存在着相反的相互依赖关系--如果提高输出的查全率，就会降低其查准率，反之亦然。

以P作为纵轴，R作为横轴作图，简称"P-R曲线"。若一个学习方法的P-R曲线完全包括另外一个，则断言此学习方法更优秀。